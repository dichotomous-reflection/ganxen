import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
import torch.optim as optim
from time import time
from os import listdir
from numpy import asarray
from numpy import vstack
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import load_img
from numpy import savez_compressed
import torch
from torchvision import transforms
from PIL import Image
from numpy import load
from matplotlib import pyplot
import argparse
import itertools
import os
import random

import torch.backends.cudnn as cudnn
import torch.utils.data
import torchvision.transforms as transforms
import torchvision.utils as vutils
from PIL import Image
from tqdm import tqdm
from torchvision.utils import save_image
from IPython.display import clear_output
import matplotlib.pyplot as plt
import numpy as np

from models import SegNet
from models import ConvNet
from loading data import load_images
from utils import ReplayBuffer
from utils import DecayLR
from utils import weights_init
from saving import save_models
from saving import shownsave

        
disc1 = ConvolNet().to(device)
disc2 = ConvolNet().to(device)
gen1 = SegNet().to(device)
gen2 = SegNet().to(device)

gen1.apply(weights_init)
gen2.apply(weights_init)


        
 
 


def train(data_A, data_B):

    cycle_loss = torch.nn.L1Loss().to(device)
    identity_loss = torch.nn.L1Loss().to(device)
    adversarial_loss = torch.nn.MSELoss().to(device)
    
    
    
    optimizer_gen = torch.optim.Adam(itertools.chain(gen1.parameters(), gen2.parameters()),
                                  lr=3e-5, betas=(0.5, 0.999))
    optimizer_disc2 = torch.optim.Adam(disc2.parameters(), lr=3e-5, betas=(0.5, 0.999))
    optimizer_disc1 = torch.optim.Adam(disc1.parameters(), lr=3e-5, betas=(0.5, 0.999))
    
    epochs=200
    decay_epochs=100
    lr_lambda = DecayLR(epochs, 0, decay_epochs).step
    lr_scheduler_gen = torch.optim.lr_scheduler.LambdaLR(optimizer_gen, lr_lambda=lr_lambda)
    lr_scheduler_disc2 = torch.optim.lr_scheduler.LambdaLR(optimizer_disc2, lr_lambda=lr_lambda)
    lr_scheduler_disc1 = torch.optim.lr_scheduler.LambdaLR(optimizer_disc1, lr_lambda=lr_lambda)
    
    g_losses = []
    d_losses = []

    identity_losses = []
    gan_losses = []
    cycle_losses = []

    fake_A_buffer = ReplayBuffer()
    fake_B_buffer = ReplayBuffer()
    epochs_num=200
    dataset=list(zip(data_A,data_B))
    num=0
    for epoch in range(0, epochs_num):
        for datA, datB in dataset:
            real_A = datA.to(device)
            real_B = datB.to(device)
            
            batch_size = 1
            num+=1
            real_label = torch.full((batch_size, 1), 1, device=device, dtype=torch.float32)
            fake_label = torch.full((batch_size, 1), 0, device=device, dtype=torch.float32)
           
            ##############################################
            
            ##############################################

            
            optimizer_gen.zero_grad()

            id_A = gen2(real_A)
            loss_id_A = identity_loss(id_A, real_A) * 5.0
            
            id_B = gen1(real_B)
            loss_id_B = identity_loss(id_B, real_B) * 5.0
            
            
            fake_A = gen2(real_B)
            fake_out_A = disc2(fake_A)
            loss_GAN_B2A = adversarial_loss(fake_out_A, real_label)
           
            fake_B = gen1(real_A)
            fake_out_B = disc1(fake_B)
            loss_GAN_A2B = adversarial_loss(fake_out_B, real_label)
            
            rec_A = gen2(fake_B)
            loss_cyc_ABA = cycle_loss(rec_A, real_A) * 10.0

            rec_B = gen1(fake_A)
            loss_cyc_BAB = cycle_loss(rec_B, real_B) * 10.0
            errG = loss_id_A + loss_id_B + loss_GAN_A2B + loss_GAN_B2A + loss_cyc_ABA + loss_cyc_BAB

            errG.backward()
            
            optimizer_gen.step()
            #########################################
            
            optimizer_disc2.zero_grad()

            # Real A image loss
            real_out_A = disc2(real_A)
            errD_real_A = adversarial_loss(real_out_A, real_label)

            # Fake A image loss
            fake_A = fake_A_buffer.push_and_pop(fake_A)
            fake_out_A = disc2(fake_A.detach())
            errD_fake_A = adversarial_loss(fake_out_A, fake_label)

            errD_A = (errD_real_A + errD_fake_A) / 2

            
            errD_A.backward()
            optimizer_disc2.step()

            ##############################################
            
            optimizer_disc1.zero_grad()

            # Real B image loss
            real_out_B = disc1(real_B)
            errD_real_B = adversarial_loss(real_out_B, real_label)

            # Fake B image loss
            fake_B = fake_B_buffer.push_and_pop(fake_B)
            fake_out_B = disc1(fake_B.detach())
            errD_fake_B = adversarial_loss(fake_out_B, fake_label)

           
            errD_B = (errD_real_B + errD_fake_B) / 2

            
            errD_B.backward()
            
            optimizer_disc1.step()
            ############################################
            
            clear_output(wait=True)
            if num%2 == 0:
                fake_image_A = 0.5 * (gen2(real_B).data + 1.0)
                fake_image_B = 0.5 * (gen1(real_A).data + 1.0)

                shownsave(real_A, fake_A, "B2A", epoch)
                shownsave(real_B, fake_B, "A2B", epoch)
                
        if epoch%17==0:
            save_models(epoch, gen1, gen2)

            print("Epoch ,", epoch, " loss_gan", loss_cycle_ABA + loss_cycle_BAB)
            
train(data_A, data_B)
